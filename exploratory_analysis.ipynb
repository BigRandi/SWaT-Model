{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc6cd31f-c140-45a3-840b-2764c844ea8b",
   "metadata": {},
   "source": [
    "# Exploratory Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587e1c49-5636-4efe-9684-2958a9b79486",
   "metadata": {},
   "source": [
    "This code imports zipped data files from an S3 bucket, determines the shape and size of the data frame (Pandas), detects outliers/NAN/Null values, calculates collumn-wise mean/median/mode/stddev, and performs some correlation analysis on the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246cccc1-f395-4ef9-ac67-a00f8bd3b638",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dependencies & Globals\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "00bc80b5-a9a2-44ca-8386-a4b24d4398e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip install hvplot"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4145194e-4d01-43a2-9fb9-85fb287776e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55e536a4-13e2-4728-b479-056878f9af8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from violin_plot import violin_plot\n",
    "import matplotlib.pyplot as plt\n",
    "import hvplot.pandas\n",
    "import seaborn as sns\n",
    "import importlib\n",
    "\n",
    "#Globals\n",
    "filename_prefix = 'normal_02_'    # Filename prefix, for storing artifacts\n",
    "store_artifacts = True            # True will save figures, tables, and dataframes to \n",
    "redownload_dataset = False         # True will redownload a copy of the dataset from S3. False uses the local Jupyter copy\n",
    "\n",
    "#data_filename = \"SWaT_Dataset_Attack_v1_01.parquet\" # Input dataset filename name here\n",
    "#data_filename = \"SWaT_Dataset_Attack_v1_02.parquet\"\n",
    "#data_filename = \"SWaT_Dataset_Normal_v1_01.parquet\"\n",
    "data_filename = \"SWaT_Dataset_Normal_v1_02.parquet\"\n",
    "\n",
    "# Root bucket for this project and location of artifact storage\n",
    "bucket = \"exploratoryanalysis-swat\"             # Bucket location for working directory\n",
    "prefix = \"analysis_artifacts\"                   # Folder location for working directory\n",
    "folder = \"normal_02\"\n",
    "\n",
    "# S3 bucket where the original data is downloaded and stored\n",
    "downloaded_data_bucket = \"exploratoryanalysis-swat\"    # Bucket location for data directory\n",
    "downloaded_data_prefix = \"dataset_unzipped\"            # Folder location of data files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2fce03-d2f7-4979-b958-c1edcf5c1f06",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Verify Bucket Connection\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe61d59d-41f9-4d95-9123-84550af8548f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input/output will be stored in: s3://exploratoryanalysis-swat/analysis_artifacts/normal_02\n",
      "Downloaded training data will be read from s3://exploratoryanalysis-swat/dataset_unzipped\n"
     ]
    }
   ],
   "source": [
    "# Housekeeping, sagemaker gets execution role and reigon from this sagemaker instance\n",
    "execution_role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# Check bucket existence and permissions\n",
    "def check_bucket_permission(bucket):\n",
    "    # check if the bucket exists\n",
    "    permission = False\n",
    "    try:\n",
    "        boto3.Session().client(\"s3\").head_bucket(Bucket=bucket)\n",
    "    except botocore.exceptions.ParamValidationError as e:\n",
    "        print(\n",
    "            \"Hey! You either forgot to specify your S3 bucket\"\n",
    "            \" or you gave your bucket an invalid name!\"\n",
    "        )\n",
    "    except botocore.exceptions.ClientError as e:\n",
    "        if e.response[\"Error\"][\"Code\"] == \"403\":\n",
    "            print(f\"Hey! You don't have permission to access the bucket, {bucket}.\")\n",
    "        elif e.response[\"Error\"][\"Code\"] == \"404\":\n",
    "            print(f\"Hey! Your bucket, {bucket}, doesn't exist!\")\n",
    "        else:\n",
    "            raise\n",
    "    else:\n",
    "        permission = True\n",
    "    return permission\n",
    "\n",
    "if check_bucket_permission(bucket):\n",
    "    print(f\"Training input/output will be stored in: s3://{bucket}/{prefix}/{folder}\")\n",
    "if check_bucket_permission(downloaded_data_bucket):\n",
    "    print(f\"Downloaded training data will be read from s3://{downloaded_data_bucket}/{downloaded_data_prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1621dc17-9437-4743-8395-4c6bc54b2710",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Download Datafile or Open Existing Parquet Datafile\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "71fea88f-5825-4107-9184-fa66dd5f67f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "if redownload_dataset == True:  \n",
    "    s3.download_file(downloaded_data_bucket, f\"{downloaded_data_prefix}/{data_filename}\", data_filename)    # Download S3 data file to Sagemaker space\n",
    "    \n",
    "# Read file into Pandas Dataframe \n",
    "sensor_data = pd.read_parquet(data_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216f6536-0926-4a8c-b05c-c8ae53185b2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Check for NAN Values\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b7dac741-e1c9-42de-bf5f-31866e848e6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Locate NAN values and store indicies\n",
    "nan_values = sensor_data[sensor_data.isna().any(axis=1)]\n",
    "if nan_values.empty:\n",
    "    pass\n",
    "else:\n",
    "    # Determine what we want to do with the missing fields\n",
    "    print('There are NAN values in the data')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e4d8f9f5-9e59-45a6-b0b2-00d1e40da762",
   "metadata": {
    "tags": []
   },
   "source": [
    "# -----------------------------THIS SECTION CAN BE REMOVED ONCE ALL FILES ARE THOROUGHLY CLEANED ------------------#\n",
    "# The files last column is labled Normal/Attack and will now just be called \"Attack\" with a boolean true/false instead of strings\n",
    "\n",
    "sensor_data = sensor_data.rename(columns={\"Normal/Attack\": \"Attack\"})\n",
    "sensor_data['Attack'] = sensor_data['Attack'].map({'Attack': True, 'Normal': False})\n",
    "sensor_data.rename(columns={' MV101':'MV101',' AIT201': 'AIT201',' MV201': 'MV201',' P201': 'P201',' P202': 'P202',' P204': 'P204',' MV303': 'MV303'}, inplace=True)\n",
    "\n",
    "filename = data_filename\n",
    "sensor_data.to_parquet(filename)\n",
    "\n",
    "s3.upload_file(\n",
    "    Filename=filename,\n",
    "    Bucket=bucket,\n",
    "    Key=f\"{prefix}/{folder}/{filename}\"\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d34b72-e2d3-4421-8c4c-c7d809ddf43c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Statistics\n",
    "## Mean, Median, Std Dev, Unique Values, etc.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a38fb3c-e9fb-44ab-b79c-77ac9fcd6efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Determine mean, median, std dev, etc. of each collumn\n",
    "data_statistics = sensor_data.describe()\n",
    "#print(\"Dataset statistics for each column:\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "#display(data_statistics)\n",
    "\n",
    "if store_artifacts == True:\n",
    "    filename = filename_prefix + 'data_statistics.csv'\n",
    "    data_statistics.to_csv(filename, index=True)\n",
    "\n",
    "    s3.upload_file(\n",
    "        Filename=filename,\n",
    "        Bucket=bucket,\n",
    "        Key=f\"{prefix}/{folder}/{filename}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7213c8-d3ea-4831-acba-339dae0c27c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Count of Unique Values per Column, Constant Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b715185c-f54f-4b96-9357-f09c33727c96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What are the unique values and counts for each categorical column\n",
    "unique_values = sensor_data.nunique(axis=0)\n",
    "#display(unique_values)\n",
    "\n",
    "if store_artifacts == True:\n",
    "    filename = filename_prefix + 'unique_values_count.csv'\n",
    "    unique_values.to_csv(filename, index=True)\n",
    "\n",
    "    s3.upload_file(\n",
    "        Filename=filename,\n",
    "        Bucket=bucket,\n",
    "        Key=f\"{prefix}/{folder}/{filename}\"\n",
    "    )\n",
    "\n",
    "# Which columns have only a single unique value? (Constants)\n",
    "constant_columns = unique_values.where(unique_values==1).dropna(how='all')\n",
    "#display(constant_columns)\n",
    "\n",
    "if store_artifacts == True:\n",
    "    filename = filename_prefix + 'constant_values.csv'\n",
    "    constant_columns.to_csv(filename, index=True)\n",
    "\n",
    "    s3.upload_file(\n",
    "        Filename=filename,\n",
    "        Bucket=bucket,\n",
    "        Key=f\"{prefix}/{folder}/{filename}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc9507f-2398-4890-b2bd-0854aeb3b872",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Outliers\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "037e2f7a-4f31-43c5-9bf5-2ce6072a0fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/scipy/stats/stats.py:2419: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (a - mns) / sstd\n"
     ]
    }
   ],
   "source": [
    "# Locate and classify outliers\n",
    "# The collumns include two objects: a timestamp and a string. These cannot be analyzed for outliers, and will be removed by selecting only columns with numbers\n",
    "numerical_sensor_data = sensor_data.select_dtypes(include='number')\n",
    "\n",
    "# Z Score tells us how many standard deviations each value is from the mean. Anything outside of 3 STD deviations is an outlier and anything outside 2 is in the farthest 5% and could be cut\n",
    "z_scores = numerical_sensor_data.apply(stats.zscore)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "#display(z_scores.head())\n",
    "\n",
    "if store_artifacts == True:\n",
    "    filename = filename_prefix + 'z_scores_matrix.parquet'\n",
    "    z_scores.to_parquet(filename)\n",
    "\n",
    "    s3.upload_file(\n",
    "        Filename=filename,\n",
    "        Bucket=bucket,\n",
    "        Key=f\"{prefix}/{folder}/{filename}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "da86ef59-61ed-418c-98cd-acc4e2ec46d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interquartile Range (IQR) is another way to classify outliers. \n",
    "# IQR determines the range between the 25th and 75th percentile\n",
    "# Then, anything that lies 1.5*(range) above or below these quartiles is considered an outlier\n",
    "# If the data value is lower than the lower range or larger than the upper range, store the index of the value.\n",
    "# Then count the number of outliers\n",
    "\n",
    "IQR = data_statistics.loc['75%'] - data_statistics.loc['25%']\n",
    "IQR = IQR.to_frame()\n",
    "IQR = IQR.transpose()\n",
    "\n",
    "# Define lower and upper ranges for each variable. Statistics does not run on timestamp or string objects. Only columns with numerical values.\n",
    "lower_range = data_statistics.loc['25%'] - 1.5 * IQR\n",
    "upper_range = data_statistics.loc['75%'] + 1.5 * IQR\n",
    "\n",
    "# The format of a query needs to be \"column label <query condition> @<variable name to be compared against>\"\n",
    "# 'FIT101<@aa' where aa is the variable that holds the limit value for this iteration\n",
    "lower_queries = ['{}<@aa'.format(k) for k in lower_range.columns]\n",
    "upper_queries = ['{}>@aa'.format(k) for k in upper_range.columns]\n",
    "\n",
    "jj = 0\n",
    "tot_upper_outl = 0\n",
    "tot_lower_outl = 0\n",
    "for column in IQR:\n",
    "    # Lower boundary check\n",
    "    aa = lower_range.iat[0,jj]                                        # ex: FIT101\n",
    "    lower_outliers = numerical_sensor_data.query(lower_queries[jj])   # ex: 'FIT101<@aa'\n",
    "    if not lower_outliers.empty:\n",
    "        #print('Condition that triggered the IQR outlier: ', lower_queries[jj], aa)\n",
    "        #print('Number of \"less than\" outliers', lower_outliers.shape[0], '\\n')\n",
    "        tot_lower_outl += lower_outliers.shape[0]\n",
    "    \n",
    "    # Upper Boundary Check\n",
    "    aa = upper_range.iat[0,jj]                                        # ex: FIT101\n",
    "    upper_outliers = numerical_sensor_data.query(upper_queries[jj])   # ex: 'FIT101>@aa'\n",
    "    if not upper_outliers.empty:\n",
    "        #print('Condition that triggered the IQR outlier: ', upper_queries[jj], aa)\n",
    "        #print('Number of \"greater than\" outliers', upper_outliers.shape[0], '\\n')\n",
    "        tot_upper_outl += upper_outliers.shape[0]\n",
    "    jj += 1\n",
    "    \n",
    "#print('Total number of outliers: ', tot_lower_outl+tot_upper_outl)\n",
    "\n",
    "if store_artifacts == True:\n",
    "    filename = filename_prefix + 'lower_IQR_outliers.parquet'\n",
    "    lower_outliers.to_parquet(filename)\n",
    "\n",
    "    s3.upload_file(\n",
    "        Filename=filename,\n",
    "        Bucket=bucket,\n",
    "        Key=f\"{prefix}/{folder}/{filename}\"\n",
    "    )\n",
    "    \n",
    "    filename = filename_prefix + 'upper_IQR_outliers.parquet'\n",
    "    upper_outliers.to_parquet(filename)\n",
    "\n",
    "    s3.upload_file(\n",
    "        Filename=filename,\n",
    "        Bucket=bucket,\n",
    "        Key=f\"{prefix}/{folder}/{filename}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326425c2-25fd-4437-a2bc-aeb680316ab9",
   "metadata": {},
   "source": [
    "# Correlation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b8851e01-ee0d-4e3d-806d-56bb9e92c72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "# First, drop all the columns with constant values. These will not impact a correlation matrix.\n",
    "variable_sensor_data = sensor_data.drop(constant_columns.index, axis=1)\n",
    "\n",
    "correlation_matrix = variable_sensor_data.corr()\n",
    "pd.set_option('display.max_columns', None)\n",
    "#display(correlation_matrix.head())\n",
    "\n",
    "# Upload correlation matrix CSV to S3 Artifacts\n",
    "if store_artifacts == True:\n",
    "    filename = filename_prefix + 'correlation_matrix.csv'\n",
    "    correlation_matrix.to_csv(filename, index=True)\n",
    "\n",
    "    s3.upload_file(\n",
    "        Filename=filename,\n",
    "        Bucket=bucket,\n",
    "        Key=f\"{prefix}/{folder}/{filename}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9f7dcb5f-b416-42da-b4f5-5eb11ec18fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation matrix\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Show all ticks and label them with the respective list entries\n",
    "ax.set_xticks(np.arange(len(correlation_matrix)));\n",
    "ax.set_yticks(np.arange(len(correlation_matrix)));\n",
    "ax.minorticks_off()\n",
    "ax.grid(which='major', \n",
    "        color='dimgrey', \n",
    "        linestyle='-', \n",
    "        linewidth=1, \n",
    "        visible=True);\n",
    "ax.set_xticklabels(correlation_matrix.columns);\n",
    "ax.set_yticklabels(correlation_matrix.index);\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), \n",
    "         rotation=45, \n",
    "         ha=\"right\",\n",
    "         rotation_mode=\"anchor\");\n",
    "\n",
    "title = filename_prefix + \"correlation_matrix\"\n",
    "\n",
    "ax.set_title(title)\n",
    "fig.set_figwidth(10);\n",
    "fig.set_figheight(10);\n",
    "fig.tight_layout();\n",
    "im = ax.imshow(correlation_matrix, cmap='seismic',vmin=-1,vmax=1);\n",
    "\n",
    "cbar = ax.figure.colorbar(im, ax=ax, shrink=0.5);\n",
    "cbar.ax.set_ylabel('Correlation Score', rotation=-90, va=\"bottom\");\n",
    "plt.tight_layout();\n",
    "#plt.show()\n",
    "\n",
    "fig\n",
    "\n",
    "if store_artifacts == True:\n",
    "    filename = title + '.png'\n",
    "    plt.savefig(filename)\n",
    "    s3.upload_file(\n",
    "        Filename=filename,\n",
    "        Bucket=bucket,\n",
    "        Key=f\"{prefix}/{folder}/{filename}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec9e55b-bb85-419d-82ad-ca83886f299e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4edd9b-f6db-44cd-a5dc-68930092a8da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Boxplots"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4785f54-0986-4c40-8e06-78525bd27ec0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Box Plot\n",
    "flierprops = dict(marker='*',\n",
    "                  markeredgecolor='red',\n",
    "                  markersize=2,\n",
    "                  linestyle='none');\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "fig1, ax1 = plt.subplots();\n",
    "ax1.set_title(filename_prefix + 'Tank Levels');\n",
    "\n",
    "ax1.boxplot([sensor_data.LIT101, sensor_data.LIT301, sensor_data.LIT401],\n",
    "            showmeans=True,\n",
    "            flierprops=flierprops);\n",
    "ax1.set_xticklabels(['LIT101','LIT301','LIT401']);\n",
    "fig1.set_figwidth(10);\n",
    "fig1.set_figheight(10);\n",
    "plt.show(fig1)\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.set_title(filename_prefix + 'Chemistry Sensors');\n",
    "ax2.boxplot([sensor_data.AIT201, sensor_data.AIT202, sensor_data.AIT203,\n",
    "            sensor_data.AIT401, sensor_data.AIT402, sensor_data.AIT501,\n",
    "            sensor_data.AIT502, sensor_data.AIT503, sensor_data.AIT504], \n",
    "            showmeans=True, \n",
    "            flierprops=flierprops)\n",
    "ax2.set_xticklabels(['AIT201','AIT202','AIT203',\n",
    "                    'AIT401','AIT402','AIT501',\n",
    "                    'AIT502','AIT503','AIT504'])\n",
    "fig2.set_figwidth(10);\n",
    "fig2.set_figheight(10);\n",
    "plt.show(fig2)\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "fig3, ax3 = plt.subplots()\n",
    "ax3.set_title(filename_prefix + 'Flow Meters');\n",
    "ax3.boxplot([sensor_data.FIT101, sensor_data.FIT201, sensor_data.FIT301,\n",
    "            sensor_data.FIT401, sensor_data.FIT501, sensor_data.FIT502,\n",
    "            sensor_data.FIT503, sensor_data.FIT504, sensor_data.FIT601], \n",
    "            showmeans=True, \n",
    "            flierprops=flierprops)\n",
    "ax3.set_xticklabels(['FIT101','FIT201','FIT301',\n",
    "                    'FIT401','FIT501','FIT502',\n",
    "                    'FIT503','FIT504','FIT601'])\n",
    "fig3.set_figwidth(10);\n",
    "fig3.set_figheight(10);\n",
    "plt.show(fig3)\n",
    "\n",
    "# ------------------------------------------------------------------------------ #\n",
    "fig4, ax4 = plt.subplots()\n",
    "ax4.set_title(filename_prefix + 'Chemistry Sensors');\n",
    "ax4.boxplot([sensor_data.AIT501, \n",
    "            sensor_data.AIT504], \n",
    "            showmeans=True, \n",
    "            flierprops=flierprops)\n",
    "ax4.set_xticklabels(['AIT501',\n",
    "                    'AIT504'])\n",
    "fig4.set_figwidth(10);\n",
    "fig4.set_figheight(10);\n",
    "plt.show(fig4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f711609-559f-4ece-bb49-b557e370e011",
   "metadata": {},
   "source": [
    "## Violin Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6bfbbb66-95c3-451d-b9a9-06b9dcc34cc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Violin plot shows the distribution of the data. Here I show the attack vs normal on the same plot\n",
    "# I had to add a row full of empty data to get the violin to do the split\n",
    "variable_sensor_data[\"all\"]=\"\"    # We need to add an empty column for the split violin plot to work.\n",
    "    \n",
    "\n",
    "for ii in variable_sensor_data.columns:\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    if 'normal' in filename_prefix:\n",
    "        split = False\n",
    "        hue = None\n",
    "    else:\n",
    "        split = True\n",
    "        hue = 'Attack'\n",
    "    \n",
    "    if ii != (\"all\") and ii != (\"Timestamp\") and ii != (\"Attack\"):\n",
    "        title = filename_prefix + \"violin_\" + ii\n",
    "        ax = violin_plot(df= variable_sensor_data, \n",
    "                         title= title, \n",
    "                         y= ii,\n",
    "                         scale = 'count',\n",
    "                         fig_width = 5,\n",
    "                         fig_height = 5,\n",
    "                         inner = None,\n",
    "                         split = split,\n",
    "                         hue = hue);\n",
    "        #plt.show(ax)\n",
    "        \n",
    "        if store_artifacts == True:\n",
    "            filename = title + '.png'\n",
    "            #fig\n",
    "            plt.savefig(filename)\n",
    "            s3.upload_file(\n",
    "                Filename=filename,\n",
    "                Bucket=bucket,\n",
    "                Key=f\"{prefix}/{folder}/{filename}\"\n",
    "            )\n",
    "        plt.close('all')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1838bf7c-155f-40b9-8630-188cf3045f56",
   "metadata": {},
   "source": [
    "## Time Series Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba957ff-2057-4340-aac1-8fda86339ecb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Using MatPlotLib"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3738ec9b-0f00-4fd3-82e9-fcf114954e37",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Plot time-series data for visual representation\n",
    "attack_data = sensor_data.query('Attack==True')\n",
    "normal_data = sensor_data.query('Attack==False', inplace=False)\n",
    "\n",
    "n_points = 100\n",
    "\n",
    "title = filename_prefix + \"instert title\"\n",
    "\n",
    "fig1, ax1 = plt.subplots();\n",
    "plt.plot(normal_data.Timestamp.loc[1:n_points], normal_data.AIT402.loc[1:n_points], 'g-');\n",
    "plt.plot(attack_data.Timestamp.loc[1:n_points], attack_data.AIT402.loc[1:n_points], 'r-');\n",
    "ax1.set_title(title);\n",
    "fig1.set_figwidth(10);\n",
    "fig1.set_figheight(10);\n",
    "\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False) # labels along the bottom edge are off\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if store_artifacts == True:\n",
    "    filename = title + '.png'\n",
    "    plt.savefig(filename)\n",
    "    s3.upload_file(\n",
    "        Filename=filename,\n",
    "        Bucket=bucket,\n",
    "        Key=f\"{prefix}/{folder}/{filename}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70f0f61-cb72-4151-826a-f5c8d15e2014",
   "metadata": {},
   "source": [
    "Using Seaborn"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ca34f04-3ea9-44c5-a458-db49ca59ef1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "n_points = sensor_data.shape[0]/1000\n",
    "\n",
    "z_scores[\"Timestamp\"]=sensor_data.Timestamp\n",
    "z_scores[\"Attack\"]=sensor_data.Attack\n",
    "\n",
    "\n",
    "#ax = sns.lineplot(data=sensor_data.loc[1:n_points], x='Timestamp', y=\"AIT402\", hue=\"Attack\")\n",
    "ax = sns.lineplot(data=z_scores.loc[1:n_points], x='Timestamp', y=\"P402\", hue=\"Attack\")\n",
    "\n",
    "#[\"AIT402\",\"P206\",\"FIT504\"]\n",
    "\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False);# labels along the bottom edge are off\n",
    "\n",
    "plt.show()\n",
    "\n",
    "title = filename_prefix + \"AIT402\"\n",
    "\n",
    "if store_artifacts == True:\n",
    "    filename = title + '.png'\n",
    "    plt.savefig(filename)\n",
    "    s3.upload_file(\n",
    "        Filename=filename,\n",
    "        Bucket=bucket,\n",
    "        Key=f\"{prefix}/{folder}/{filename}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670cca15-85a1-47cd-9ae6-2245fa822177",
   "metadata": {},
   "source": [
    "Using HVPLOT (and Bokeh)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b11a6400-5de0-48ce-88e6-cfcd556cc927",
   "metadata": {},
   "source": [
    "def hook(plot, element):\n",
    "    plot.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom=False,      # ticks along the bottom edge are off\n",
    "    top=False,         # ticks along the top edge are off\n",
    "    labelbottom=False) # labels along the bottom edge are off"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6bf00b53-1841-4055-98be-ca342cfb1b01",
   "metadata": {
    "tags": []
   },
   "source": [
    "n_points = sensor_data.shape[0]/10\n",
    "graph = sensor_data.loc[1:n_points].hvplot.line(x='Timestamp', y='AIT402')#, xticks=20) #there is a bug in HVPLOT where xticks does not work. It will not output anything\n",
    "graph"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
